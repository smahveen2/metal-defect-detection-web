# -*- coding: utf-8 -*-
"""final_model.h5

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KdnDz466Mes5XtREb2SFTSFOdzxO_xq_
"""

import tensorflow as tf
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
from tensorflow.keras.preprocessing.image import load_img, img_to_array
import matplotlib.pyplot as plt
import numpy as np
import os
from sklearn.metrics import confusion_matrix
import seaborn as sns

# Set up paths
train_dir = '/content/sample_data/train'
test_dir = '/content/sample_data/test'
valid_dir = '/content/sample_data/valid'

# Verify directories exist
print("Train dir exists:", os.path.exists(train_dir))
print("Test dir exists:", os.path.exists(test_dir))
print("Valid dir exists:", os.path.exists(valid_dir))

# Get class names from subdirectories in the training directory
class_names = os.listdir(train_dir)  # Assuming subdirectories represent classes

# Calculate number of classes
num_classes = len(class_names)

# Function to load images and labels from directory with debugging
def load_data_from_directory(directory, target_size=(256, 256)):
    images = []
    labels = []
    class_names = sorted(os.listdir(directory))
    class_names = [name for name in class_names if os.path.isdir(os.path.join(directory, name))]

    print(f"\nProcessing directory: {directory}")
    print(f"Found classes: {class_names}")

    for class_idx, class_name in enumerate(class_names):
        class_dir = os.path.join(directory, class_name)
        print(f"  Class: {class_name}")
        files_found = 0

        for filename in os.listdir(class_dir):
            if filename.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp')):  # Added .bmp and case-insensitive check
                img_path = os.path.join(class_dir, filename)
                try:
                  # Load and preprocess image
                    img = load_img(img_path, target_size=target_size)
                    img_array = img_to_array(img) / 255.0  # Normalize to [0,1]
                    images.append(img_array)
                    # Create one-hot encoded label
                    label = tf.keras.utils.to_categorical(class_idx, num_classes=len(class_names))
                    labels.append(label)
                    files_found += 1
                except Exception as e:
                    print(f"    Error loading {img_path}: {str(e)}")
            else:
                print(f"    Skipping {filename} - not a recognized image format")

        print(f"    Found {files_found} images in {class_name}")

    if not images:
        print(f"WARNING: No images loaded from {directory}")

    return np.array(images), np.array(labels), class_names
# Load the datasets
x_train, y_train, class_names = load_data_from_directory(train_dir)
x_val, y_val, _ = load_data_from_directory(valid_dir)
x_test, y_test, _ = load_data_from_directory(test_dir)

# Print shapes and diagnostics
print("\nx_train shape:", x_train.shape)
print("y_train shape:", y_train.shape)
print("x_val shape:", x_val.shape)
print("y_val shape:", y_val.shape)
print("x_test shape:", x_test.shape)
print("y_test shape:", y_test.shape)
print("Class names:", class_names)

from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
import matplotlib.pyplot as plt

# Data augmentation layer
data_augmentation = tf.keras.Sequential([
    tf.keras.layers.RandomFlip("horizontal"),
    tf.keras.layers.RandomRotation(0.2),
    tf.keras.layers.RandomZoom(0.2),
    tf.keras.layers.RandomTranslation(0.2, 0.2),
    tf.keras.layers.RandomContrast(0.2)
])

# Build CNN model
model = tf.keras.Sequential([
    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(256, 256, 3), padding='same'),
    tf.keras.layers.MaxPooling2D((2, 2)),

    tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'),
    tf.keras.layers.MaxPooling2D((2, 2)),

    tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same'),
    tf.keras.layers.MaxPooling2D((2, 2)),

    tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same'),
    tf.keras.layers.MaxPooling2D((2, 2)),

    tf.keras.layers.Conv2D(512, (3, 3), activation='relu', padding='same'),
    tf.keras.layers.MaxPooling2D((2, 2)),

    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(512, activation='relu'),
    tf.keras.layers.Dropout(0.),
    tf.keras.layers.Dense(num_classes, activation='softmax')
])

model.summary()

# Compile with optimized Adam
optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
model.compile(
    optimizer=optimizer,
    loss='categorical_crossentropy',
    metrics=['accuracy', 'Precision', 'Recall']
)

# Callbacks
callbacks = [
    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),
    ModelCheckpoint('best_model.keras', monitor='val_accuracy', save_best_only=True, mode='max'),
    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6)
]

history = model.fit(
        x_train, y_train,
        epochs=25,
        batch_size=64,
        validation_data=(x_val, y_val),
        callbacks=callbacks
    )

# Save the final model
model.save('final_model.keras')

# Training Visualization

y_pred_probs = model.predict(x_test)
y_pred = np.argmax(y_pred_probs, axis=1)  # Convert softmax probabilities to class indices
y_true = np.argmax(y_test, axis=1)

def plot_training_history(history):
    plt.figure(figsize=(15, 5))

    plt.subplot(1, 3, 1)
    plt.plot(history.history['accuracy'])
    plt.plot(history.history['val_accuracy'])
    plt.title('Model Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend(['Train', 'Validation'])

    plt.subplot(1, 3, 2)
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('Model Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend(['Train', 'Validation'])

    # Confusion Matrix plot
    cm = confusion_matrix(y_true, y_pred)
    plt.subplot(1, 3, 3)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
    plt.title('Confusion Matrix')
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')

    plt.show()

plot_training_history(history)

import numpy as np
import matplotlib.pyplot as plt

def visualize_predictions(model, x_test, y_test, class_names):
    predictions = model.predict(x_test)  # Predict all test samples
    predicted_labels = np.argmax(predictions, axis=1)
    true_labels = np.argmax(y_test, axis=1)

    unique_classes = set()
    selected_indices = []

    # Find one example per class
    for i in range(len(true_labels)):
        class_label = true_labels[i]
        if class_label not in unique_classes:
            unique_classes.add(class_label)
            selected_indices.append(i)
        if len(unique_classes) == len(class_names):  # Stop when we have all classes
            break

        # Plot predictions for selected indices
    plt.figure(figsize=(len(class_names) * 2, 3))
    for idx, sample_idx in enumerate(selected_indices):
        plt.subplot(1, len(class_names), idx + 1)
        plt.imshow(x_test[sample_idx])
        pred_class = class_names[predicted_labels[sample_idx]]
        true_class = class_names[true_labels[sample_idx]]
        plt.title(f"Pred: {pred_class}\nTrue: {true_class}")
        plt.axis('off')

    plt.show()

visualize_predictions(model, x_test, y_test, class_names)

import tensorflow as tf

# ... (Your model definition and training code from previous cells) ...

# Define your model architecture (if not already defined)
model = tf.keras.Sequential([
    # ... your model layers ...
])

# ... (Compile and train your model) ...

# Save the model
model.save('final_model.h5')

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
from tensorflow.keras.datasets import cifar10
from tensorflow.keras.utils import to_categorical

# Load CIFAR-10 data as a placeholder (3 classes: airplane, car, bird)
(x_train, y_train), (x_test, y_test) = cifar10.load_data()

# Use only first 3 classes to simulate defect categories
mask = y_train.flatten() < 3
x_train = x_train[mask]
y_train = y_train[mask]

mask = y_test.flatten() < 3
x_test = x_test[mask]
y_test = y_test[mask]

# Normalize
x_train = x_train / 255.0
x_test = x_test / 255.0

# One-hot encode labels
y_train = to_categorical(y_train, 3)
y_test = to_categorical(y_test, 3)

# Build model
model = Sequential([
    Conv2D(32, (3,3), activation='relu', input_shape=(32, 32, 3)),
    MaxPooling2D(2, 2),
    Flatten(),
    Dense(64, activation='relu'),
    Dense(3, activation='softmax')  # 3 output classes
])

# Compile model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train model (quick training for testing)
model.fit(x_train, y_train, epochs=3, batch_size=32, validation_split=0.2)

# Save model to HDF5 format
model.save("final_model.h5")

print("âœ… Model saved as final_model.h5")